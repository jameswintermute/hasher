# hasher.conf — configuration for hasher project
# Place in the same directory as hasher.sh (or pass with --config FILE)
# Lines beginning with # are comments.

[about]
# Optional metadata (for log banners / documentation)
project   = Hasher — NAS File Hasher & Duplicate Finder
version   = v1.0.0
copyright = (C) 2025 James Wintermute <jameswinter@protonmail.ch>
license   = GNU GPLv3 (https://www.gnu.org/licenses/)

[setup]
# First-Time-Run (FTR) helper
# If true and this is the first run (no hashes yet), hasher.sh checks your
# pathfile and prompts with a friendly guide if it’s missing/empty.
first_run_help  = true
# Where to write a starter paths template if needed
paths_template  = paths.example.txt

[logging]
# Interval (seconds) between progress updates in logs
background-interval = 15
# Log level: debug | info | warn | error
level = info
# Enable bash xtrace debugging (set true only when troubleshooting)
xtrace = false

[exclusions]
# Inherit built-in NAS/OS junk file excludes (recommended: true)
inherit-defaults = true

# Add your own exclusion patterns here (one per line).
# Shell globs are supported. Examples:
*.tmp
*.part
*.bak
*/Cache/*
*/.Trash*/**

[review]
# Defaults for duplicate review and summary scripts (find-duplicates.sh / review-duplicates.sh)

# Which input file strategy to use when none specified:
#   latest  → auto-select the most recent hasher-*.csv
#   prompt  → interactively select from the 10 most recent
#   <file>  → explicit filename (relative to hashes/)
input = latest

# Sorting of duplicate groups:
#   count_desc → highest file counts first
#   size_desc  → largest total size first
#   hash_asc   → alphabetical by hash
sort = count_desc

# Whether to skip zero-length files in duplicate detection
skip_zero_size = true

# Minimum file size (MB) to include in duplicate review
min_size_mb = 0.00

# Regex filters (POSIX extended regex). Leave blank for none.
include_regex =
exclude_regex =

# Where to write duplicate reports and plans
report_dir = duplicate-hashes

# Prepend YYYY-MM-DD to duplicate report filenames
report_prefix_date = true

# Limit how many duplicate groups are included in the summary (0 = no limit)
summary_limit_groups = 0
